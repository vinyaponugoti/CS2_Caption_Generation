# Automatic Caption Generation for the Visually Impaired

This case study explores how machine learning can be used to help the visually impaired and explores if there limitations to the current models/data available. 

## Hook Document
Describes the prompt, data, and deliverable for the student assigned to the case study.

## Rubric
Describes the criteria that must be met for the assigned student to successfully meet expectations for the deliverable.

## Materials 
Consists of articles that provide motivational and technical context for the case study.

| Article | Description | 
| -------------- | --------- |
| "AI Could Change How Blind People See the World" | Describes the impacts of the recent advancements in AI on the lives of the visually impaired.|
| "CNN-LSTM Architecture and Image Captioning"| Describes how to use CNN and LSTM models to generate captions for images. | 

## Data
The data that will be used to train and test the model is in the data subfolder of the materials folder.

| File | Description | 
| -------------- | --------- |
| Flicker30k-images.txt | Consists of the google drive link that contains the 30k images that will be used to train the caption generation model| 
| Flicker30k-captions.txt | Consists of the correlated captions that will be used to train the model|
| Low Contrast Test Set | Consists of the low contrast images that the model will generate captions for | 
| Well Lit Test Set | Consists of the well lit images that the model will generate captions for |
| Data Dictionary Pdf| Provides the data dictionary for the data used to train the model|

## References

[1] S. Pardeshi, “CNN-LSTM architecture and image captioning,” Medium, https://medium.com/analytics-vidhya/cnn-lstm-architecture-and-image-captioning-2351fc18e8d7 (accessed Dec. 4, 2023). \
[2] K. Johnson, “Ai could change how blind people see the world,” Wired, https://www.wired.com/story/ai-gpt4-could-change-how-blind-people-see-the-world/ (accessed Dec. 4, 2023). 